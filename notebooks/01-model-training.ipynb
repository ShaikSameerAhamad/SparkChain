{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d471a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '.venv (Python 3.12.10)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings to keep the output clean\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# --- Data Loading and Preparation ---\n",
    "print(\"Loading datasets...\")\n",
    "# Define the project root relative to the notebook's location\n",
    "PROJECT_ROOT = Path.cwd().parent \n",
    "\n",
    "try:\n",
    "    # Load the datasets from the 'data' folder\n",
    "    train_df = pd.read_csv(PROJECT_ROOT / 'data' / 'train.csv')\n",
    "    features_df = pd.read_csv(PROJECT_ROOT / 'data' / 'features.csv')\n",
    "    stores_df = pd.read_csv(PROJECT_ROOT / 'data' / 'stores.csv')\n",
    "\n",
    "    # Merge the datasets into one master dataframe\n",
    "    df = train_df.merge(features_df, on=['Store', 'Date', 'IsHoliday'], how='inner')\n",
    "    df = df.merge(stores_df, on='Store', how='inner')\n",
    "    print(\"Datasets loaded and merged.\")\n",
    "\n",
    "    # --- Data Cleaning and Feature Engineering ---\n",
    "    # Convert 'Date' column to datetime objects for time-series analysis\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # For our first model, we will focus on a single store and department\n",
    "    # This keeps training fast and simple for our MVP\n",
    "    df_filtered = df[(df['Store'] == 1) & (df['Dept'] == 1)].copy()\n",
    "    print(f\"Filtered data for Store 1, Dept 1. We have {len(df_filtered)} data points.\")\n",
    "\n",
    "    # Prophet requires specific column names: 'ds' for the date and 'y' for the value\n",
    "    df_prophet = df_filtered[['Date', 'Weekly_Sales']].rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'})\n",
    "\n",
    "    # --- Model Training ---\n",
    "    print(\"Training Prophet model... This may take a minute.\")\n",
    "    # Initialize and fit the model\n",
    "    model = Prophet()\n",
    "    model.fit(df_prophet)\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # --- Save the Model ---\n",
    "    # Define the path to save our trained model\n",
    "    MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "    MODELS_DIR.mkdir(exist_ok=True) # Create the 'models' directory if it doesn't exist\n",
    "    model_path = MODELS_DIR / 'prophet_model.pkl'\n",
    "\n",
    "    # Use joblib to save the model object to a file for later use in our app\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"SUCCESS: Model has been saved to: {model_path}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Could not find a data file. {e}. Make sure the Kaggle CSV files are in the 'data' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
